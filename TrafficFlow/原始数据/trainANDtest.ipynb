{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "import datetime\n",
    "import random\n",
    "import warnings\n",
    "import joblib\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import auc, roc_curve\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from scipy import stats\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option(\"display.max_columns\", None) # show all columns\n",
    "pd.set_option(\"max_colwidth\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "par_LR = [{'C':[0.01,0.1,1,10],\n",
    "           'class_weight':['balanced'],\n",
    "           'max_iter':[5]}]\n",
    "par_RF = [{'n_estimators':[30, 60],\n",
    "            'max_depth':[30, 60],\n",
    "            'max_leaf_nodes':[30, 60],\n",
    "            #'class_weight':[scale_pos_weight],\n",
    "          }]\n",
    "par_ANN = [{'hidden_layer_sizes':[(50, 50), (100, 100), (150, 150)]}]\n",
    "par_XGB = [{'n_estimators': [30, 60], \n",
    "            'max_depth': [30, 60],\n",
    "            'reg_lambda':[0.1, 1, 10],\n",
    "            'gamma':[0.1, 1, 10],\n",
    "            #'scale_pos_weight':[scale_pos_weight],\n",
    "           }]\n",
    "par_FIN = [{'n_estimators': [60], \n",
    "            'max_depth': [30],\n",
    "            'reg_lambda':[10],\n",
    "            'gamma':[0.1],\n",
    "            #'scale_pos_weight':[scale_pos_weight],\n",
    "           }]\n",
    "\n",
    "max_count_LR = float(\"inf\")\n",
    "max_count_RF = float(\"inf\")\n",
    "max_count_ANN = float(\"inf\")\n",
    "max_count_XGB = 50\n",
    "max_count_FIN = 50\n",
    "use_LR, use_RF, use_ANN, use_XGB, use_FIN = True, False, True, True, False\n",
    "\n",
    "delimsText = \"-\" * 120\n",
    "input_data = \"d1_m8.csv\"\n",
    "split_percentage = 0.8\n",
    "train_test_range = (0, -3, -3, None) # [,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mf:\n",
    "    @staticmethod # a function to search and download tables from PSQL\n",
    "    def search_table(s = '*', f = 'admissions', w = '', like = '', equal = '', download_as = ''):\n",
    "        temp = s.lower()\n",
    "        for i in temp:\n",
    "            if i < \"a\" or i > \"z\":\n",
    "                raise TypeError\n",
    "        del temp\n",
    "        conn = psycopg2.connect(database = \"mimic\", user = \"postgres\",\n",
    "                                password = \"pandora\", host = \"127.0.0.1\", port = \"5432\")\n",
    "        cursor = conn.cursor()\n",
    "        if (w != '') & (like != \"\"):\n",
    "            cursor.execute(\"select \" + s + \" from mimic.\" + f +' where '+ w +\n",
    "                           ' like ' + \"'%\"+ like +\"%'\")\n",
    "        elif (w != '') & (equal != ''):\n",
    "            cursor.execute(\"select \" + s + \" from mimic.\" + f +' where '+ w + \"=\"\n",
    "                          +equal)\n",
    "        else:\n",
    "            cursor.execute(\"select \" + s + \" from mimic.\" + f)\n",
    "        table = cursor.fetchall()\n",
    "        if len(table) != 0:\n",
    "            colnames = [desc[0] for desc in cursor.description]\n",
    "            columns = cursor.fetchall()\n",
    "            cursor.close()\n",
    "            conn.close()\n",
    "            columns = pd.DataFrame(columns)\n",
    "            table = pd.DataFrame(table)\n",
    "            table.columns = colnames\n",
    "        if download_as != \"\":\n",
    "            table.to_csv(\"download data/\" + download_as + \".csv\", index = False)\n",
    "        return table\n",
    "    \n",
    "    \n",
    "    @staticmethod # some functions to convert time into hours\n",
    "    def calculate_time(x):\n",
    "        if x == \"nan\":\n",
    "            return \"NaN\"\n",
    "        else:\n",
    "            curTime = datetime.datetime.strptime(x, \"%Y-%m-%d %H:%M:%S\")\n",
    "            utcTime2 = datetime.datetime.strptime(\"1970-01-01 00:00:00\", \"%Y-%m-%d %H:%M:%S\")\n",
    "            metTime = curTime - utcTime2\n",
    "            timeStamp = metTime.days * 24 * 3600 + metTime.seconds\n",
    "            return (timeStamp / 3600 + 87600 + 72)\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def transform_time(dataframe, column_name):\n",
    "        dataframe[column_name + \"_h\"] = dataframe[column_name].apply(lambda x: calculate_time(str(x)))\n",
    "    \n",
    "    \n",
    "    @staticmethod # use winsorize to deal with outlier\n",
    "    def winsorize(data, percentage_one = 0.01, percentage_two = 0.99, show = False):\n",
    "        if show == True:\n",
    "            plt.figure(figsize = (5, 5), dpi = 300)\n",
    "            ax1 = plt.subplot(121)\n",
    "            plt.hist(data, bins = 100)\n",
    "        count1 = len(data)\n",
    "        lower_bound = data.quantile(percentage_one)\n",
    "        upper_bound = data.quantile(percentage_two)\n",
    "        count2 = len(data[data < lower_bound]) + len(data[data > upper_bound])\n",
    "        data[data < lower_bound] = lower_bound\n",
    "        data[data > upper_bound] = upper_bound\n",
    "    \n",
    "        if show == True:\n",
    "            print(\"{0}:\\nlower bound is {1}\\nupeer bound is {2}\\ntotally dealt with {3} of {4} ({5}%)\".format(data.name, \n",
    "                   lower_bound, upper_bound, count2, count1, (count2 / count1) * 100))\n",
    "            ax2 = plt.subplot(122)\n",
    "            plt.hist(data, bins=100)\n",
    "    \n",
    "    \n",
    "    @staticmethod # by using regression to fill Nan\n",
    "    def fill(dataframe, variables_list, variables_y, parameters, model, show = False):\n",
    "        for i in variables_y:\n",
    "            if show == True:\n",
    "                plt.figure(figsize = (10, 10), dpi = 300)\n",
    "                ax1 = plt.subplot(121)\n",
    "                plt.hist(dataframe[i], bins = 100)\n",
    "            clf = GridSearchCV(model, parameters, cv = 5)\n",
    "            X = dataframe[dataframe[i].notnull()][variables_list]\n",
    "            y = dataframe[dataframe[i].notnull()][i]\n",
    "            clf.fit(X, y)\n",
    "            scores = clf.score(X, y)\n",
    "            # scores = cross_val_score(estimator=model, X=dataframe[variables_list], y=dataframe['hospital_mortality'], cv=5,scoring = \"neg_mean_absolute_error\")\n",
    "            # train_score=model.score(dataframe[variables_list], dataframe['hospital_mortality'])\n",
    "    \n",
    "            print(\"the best model is:\", clf.best_estimator_, \"with best score\", clf.best_score_)\n",
    "            print(\"mean test scores are\", clf.cv_results_[\"mean_test_score\"])\n",
    "            fillX = dataframe[dataframe[i].isnull()][variables_list]\n",
    "            dataframe.loc[dataframe[dataframe[i].isnull()].index, i] = clf.predict(fillX)\n",
    "            if show == True:\n",
    "                ax2 = plt.subplot(122)\n",
    "                plt.hist(dataframe[i], bins = 100)\n",
    "                plt.show()\n",
    "    \n",
    "    \n",
    "    @staticmethod # k-s test\n",
    "    def ksTest(dataframe, variables, show = \"\"):\n",
    "        for i in variables:\n",
    "            res = stats.kstest(dataframe[dataframe[i].notnull()][i], \"norm\")\n",
    "            print(str(i) + \":\", res)\n",
    "            if show != \"\" and i == show:\n",
    "                plt.hist(dataframe[i], bins=50)\n",
    "                plt.show()\n",
    "    \n",
    "    \n",
    "    @staticmethod # t-test\n",
    "    def tTest(dataframe, variables, groupby, show = \"\"):\n",
    "        result = []\n",
    "        for i in variables:\n",
    "            group1 = dataframe[(dataframe[groupby] == 1) & (dataframe[i].notnull())][i]\n",
    "            group2 = dataframe[(dataframe[groupby] == 0) & (dataframe[i].notnull())][i]\n",
    "            res = stats.ttest_ind(group1, group2)\n",
    "            result.append(res)\n",
    "            print(i, \":\", res)\n",
    "            if show != \"\" and i == show:\n",
    "                fig = plt.figure(figsize = (5, 5), dpi = 300)\n",
    "                ax1 = fig.add_subplot(111)\n",
    "                ax1.hist([group1, group2], bins = 50)\n",
    "                plt.show()\n",
    "        return result\n",
    "    \n",
    "    \n",
    "    @staticmethod # chi-square test\n",
    "    def chi2Test(dataframe, variables_x, variable_y, show = False):\n",
    "        for i in variables_x:\n",
    "            count = 0\n",
    "            row = len(dataframe[i].unique())\n",
    "            column = len(dataframe[variable_y].unique())\n",
    "            chi2_data = np.zeros((row, column))\n",
    "            list1 = dataframe[i].unique()\n",
    "            list2 = dataframe[variable_y].unique()\n",
    "            for j in range(len(list1)):\n",
    "                for k in range(len(list2)):\n",
    "                    chi2_data[j, k] = len(dataframe[(dataframe[i] == list1[j])\n",
    "                                                    & (dataframe[variable_y] == list2[k])])\n",
    "                    count += len(dataframe[(dataframe[i] == list1[j])\n",
    "                                           & (dataframe[variable_y] == list2[k])])\n",
    "            res = stats.chi2_contingency(np.array(chi2_data))\n",
    "            print(\"chi2Test between {0} and {1}:\\np-value is: {2}:\\nthere are totally {3} samples\".format(i, variable_y, res[1], res[2], count))\n",
    "            if show:\n",
    "                print(\"chi2Test metrix between {0} and {1}:\\n{2}\".format(i, variable_y, chi2_data))\n",
    "    \n",
    "    \n",
    "    @staticmethod # Mann-Whitney U test\n",
    "    def wilcox(dataframe, variables, groupby, show = \"\"):\n",
    "        for j in groupby:\n",
    "            for i in variables:\n",
    "                group1 = dataframe[(dataframe[j] == 1) & (dataframe[i].notnull())][i]\n",
    "                group2 = dataframe[(dataframe[j] == 0) & (dataframe[i].notnull())][i]\n",
    "                res = stats.mannwhitneyu(group1, group2)\n",
    "                print(i, \":\", res)\n",
    "                if show != \"\" and i == show:\n",
    "                    fig = plt.figure(figsize=(5, 5), dpi=300)\n",
    "                    ax1 = fig.add_subplot(111)\n",
    "                    ax1.hist([group1, group2], bins=50)\n",
    "                    plt.show()\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def meanFill(dataframe, variables_list, show = False):\n",
    "        for i in variables_list:\n",
    "            dataframe[i].fillna(dataframe[i].mean(), inplace = True)\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def to_csv(res, fp_path):\n",
    "        dirname, filename = os.path.split(fp_path)\n",
    "        for i in \"\\\\/:*?\\\"<>|\":\n",
    "            if i in filename:\n",
    "                filename = filename.replace(i, \"_\")\n",
    "        if not os.path.exists(dirname):\n",
    "            os.makedirs(dirname)\n",
    "        res.to_csv(os.path.join(dirname, filename))\n",
    "    \n",
    "    @staticmethod\n",
    "    def dump(res, fp_path):\n",
    "        dirname, filename = os.path.split(fp_path)\n",
    "        for i in \"\\\\/:*?\\\"<>|\":\n",
    "            if i in filename:\n",
    "                filename = filename.replace(i, \"_\")\n",
    "        if not os.path.exists(dirname):\n",
    "            os.makedirs(dirname)\n",
    "        joblib.dump(res, os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "must be str, not int",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-639414decdab>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mtrain_variables\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_test_range\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mtrain_test_range\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0moutcome_variables\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_test_range\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mtrain_test_range\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mmf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmeanFill\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns_lists\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"int\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0msub_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_variables\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0moutcome_variables\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-7113f8af2504>\u001b[0m in \u001b[0;36mmeanFill\u001b[1;34m(dataframe, variables_list, show)\u001b[0m\n\u001b[0;32m    160\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmeanFill\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataframe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvariables_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshow\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvariables_list\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 162\u001b[1;33m             \u001b[0mdataframe\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataframe\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    163\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\program files\\python\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mstat_func\u001b[1;34m(self, axis, skipna, level, numeric_only, **kwargs)\u001b[0m\n\u001b[0;32m  11473\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_agg_by_level\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mskipna\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  11474\u001b[0m         return self._reduce(\n\u001b[1;32m> 11475\u001b[1;33m             \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mskipna\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumeric_only\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnumeric_only\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m  11476\u001b[0m         )\n\u001b[0;32m  11477\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\program files\\python\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m_reduce\u001b[1;34m(self, op, name, axis, skipna, numeric_only, filter_type, **kwds)\u001b[0m\n\u001b[0;32m   4247\u001b[0m                 )\n\u001b[0;32m   4248\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"ignore\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4249\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelegate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mskipna\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4250\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4251\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_reindex_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\program files\\python\\lib\\site-packages\\pandas\\core\\nanops.py\u001b[0m in \u001b[0;36m_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minvalid\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"ignore\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m                     \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m                 \u001b[1;31m# we want to transform an object array\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\program files\\python\\lib\\site-packages\\pandas\\core\\nanops.py\u001b[0m in \u001b[0;36mf\u001b[1;34m(values, axis, skipna, **kwds)\u001b[0m\n\u001b[0;32m    127\u001b[0m                     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0malt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mskipna\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 129\u001b[1;33m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0malt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mskipna\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    130\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\program files\\python\\lib\\site-packages\\pandas\\core\\nanops.py\u001b[0m in \u001b[0;36mnanmean\u001b[1;34m(values, axis, skipna, mask)\u001b[0m\n\u001b[0;32m    561\u001b[0m         \u001b[0mdtype_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    562\u001b[0m     \u001b[0mcount\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_counts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype_count\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 563\u001b[1;33m     \u001b[0mthe_sum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_ensure_numeric\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype_sum\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    564\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    565\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mthe_sum\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ndim\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\program files\\python\\lib\\site-packages\\numpy\\core\\_methods.py\u001b[0m in \u001b[0;36m_sum\u001b[1;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[0;32m     36\u001b[0m def _sum(a, axis=None, dtype=None, out=None, keepdims=False,\n\u001b[0;32m     37\u001b[0m          initial=_NoValue, where=True):\n\u001b[1;32m---> 38\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mumr_sum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwhere\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m def _prod(a, axis=None, dtype=None, out=None, keepdims=False,\n",
      "\u001b[1;31mTypeError\u001b[0m: must be str, not int"
     ]
    }
   ],
   "source": [
    "# train test data split\n",
    "data = pd.read_csv(input_data)\n",
    "columns_lists = data.columns.to_list()\n",
    "for i in range(len(columns_lists)):\n",
    "    tmp_columns = columns_lists[i]\n",
    "    for j in \"\\\\/:*?\\\"<>| \":\n",
    "        tmp_columns = tmp_columns.replace(j, \"_\")\n",
    "    columns_lists[i] = tmp_columns\n",
    "data.columns = columns_lists\n",
    "train_variables = data.columns.to_list()[train_test_range[0]:train_test_range[1]]\n",
    "outcome_variables = data.columns.to_list()[train_test_range[2]:train_test_range[3]]\n",
    "mf.meanFill(data, columns_lists)\n",
    "data = data.astype(\"int\")\n",
    "sub_data = data[list(train_variables) + outcome_variables]\n",
    "train = sub_data[:int(len(data) * split_percentage)]\n",
    "test = sub_data[int(len(data) * split_percentage):]\n",
    "name = data.columns.to_list()[train_test_range[0]:train_test_range[1]]\n",
    "y_name = data.columns.to_list()[train_test_range[2]:train_test_range[3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "data.describe()\n",
    "report = []\n",
    "count = 0\n",
    "enumerate_lists = [0.01, 0.02, 0.03, 0.04, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1, 2, 3, 4, 5, 6, \n",
    "                   7, 8, 9, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 200, 300, 400, 500] + list(range(500, 9001, 100))\n",
    "test_name = []\n",
    "file_name = []\n",
    "for i in name:\n",
    "    test_name.append(i + \".csv\")\n",
    "for i in y_name:\n",
    "    file_name.append(\"./test/\" + i + \".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "if use_LR:\n",
    "    report = []\n",
    "    count = 0\n",
    "    for time, scale_pos_weight in enumerate(enumerate_lists):\n",
    "        for j, i in enumerate(outcome_variables):\n",
    "            if count >= max_count_LR:\n",
    "                print(\"Aborted by max_count equaled\", max_count_LR)\n",
    "                break\n",
    "            count += 1\n",
    "            parameters = par_LR\n",
    "            model = GridSearchCV(LogisticRegression(n_jobs = -1), parameters, cv = 5)\n",
    "            model.fit(train[train_variables], train[i])\n",
    "            y_score = model.predict_proba(test[train_variables])[:, 1]\n",
    "            fpr, tpr, _ = roc_curve(np.array(test[i]), np.array(y_score), pos_label = 2)\n",
    "            AUROC = auc(fpr, tpr)\n",
    "            trainS = model.score(train[train_variables], train[i])\n",
    "            testS = model.score(test[train_variables], test[i])\n",
    "            f1S = metrics.f1_score(test[i], model.predict(test[train_variables]),average = \"micro\")\n",
    "            preS = metrics.precision_score(test[i], model.predict(test[train_variables]),average = \"micro\")\n",
    "            recS = metrics.recall_score(test[i], model.predict(test[train_variables]),average = \"micro\")\n",
    "            AUROC = auc(fpr, tpr)\n",
    "            #feature_importance=pd.DataFrame({'column':train_variables, 'values':model.best_estimator_.coef_})\n",
    "            #feature_importance.sort_values(by='values', axis=0, ascending=False, inplace=True)\n",
    "            report.append([trainS, testS, f1S, preS, recS, AUROC, model.best_params_, i])\n",
    "            columns = [\"trainS\", \"testS\", \"f1S\", \"preS\", \"recS\", \"AUROC\", \"model.best_params_\", \"i\"]\n",
    "            #feature_importance.iloc[0].values,feature_importance.iloc[1].values,feature_importance.iloc[2].values,\n",
    "            #feature_importance.iloc[3].values,feature_importance.iloc[4].values])\n",
    "            print(\"{0}: {1} - {2} - {3} - {4} - {5} - {6} - {7}\".format(i, trainS, testS, f1S, preS, recS, AUROC, model.best_params_))\n",
    "            print(delimsText)\n",
    "            res = pd.DataFrame(report, columns = columns)\n",
    "            mf.to_csv(res, \"./result/trained models/LR_models/report.csv\")\n",
    "            mf.dump(model, \"./result/trained models/LR_models/\" + i + \"_LR.model\")\n",
    "        if count >= max_count_LR:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "if use_RF:\n",
    "    report = []\n",
    "    count = 0\n",
    "    for time, scale_pos_weight in enumerate(enumerate_lists):\n",
    "        for i in outcome_variables:\n",
    "            if count >= max_count_RF:\n",
    "                print(\"Aborted by max_count equaled\", max_count_RF)\n",
    "                break\n",
    "            count += 1\n",
    "            parameters = par_RF[0]\n",
    "            parameters.update({\"class_weight\":[scale_pos_weight]})\n",
    "            parameters = [parameters]\n",
    "            model = GridSearchCV(RandomForestClassifier(), parameters, cv = 5)\n",
    "            model.fit(train[train_variables], train[i])\n",
    "            y_score = model.predict_proba(test[train_variables])[:, 1]\n",
    "            fpr, tpr, _ = roc_curve(np.array(test[i]), np.array(y_score), pos_label = 2)\n",
    "            AUROC = auc(fpr, tpr)\n",
    "            trainS = model.score(train[train_variables], train[i])\n",
    "            testS = model.score(test[train_variables], test[i])\n",
    "            f1S = metrics.f1_score(test[i], model.predict(test[train_variables]), average = \"micro\")\n",
    "            preS = metrics.precision_score(test[i], model.predict(test[train_variables]), average = \"micro\")\n",
    "            recS = metrics.recall_score(test[i], model.predict(test[train_variables]), average = \"micro\")\n",
    "            AUROC = auc(fpr, tpr)\n",
    "            #feature_importance = pd.DataFrame({\"column\":train_variables, \"values\":model.best_estimator_.coef_})\n",
    "            #feature_importance.sort_values(by = \"values\", axis = 0, ascending = False, inplace = True)\n",
    "            report.append([trainS, testS, f1S, preS, recS, AUROC, model.best_params_, i])\n",
    "            columns = [\"trainS\", \"testS\", \"f1S\", \"preS\", \"recS\", \"AUROC\", \"model.best_params_\", \"i\"]\n",
    "            #feature_importance.iloc[0].values,feature_importance.iloc[1].values,feature_importance.iloc[2].values,\n",
    "            #feature_importance.iloc[3].values,feature_importance.iloc[4].values])\n",
    "            print(\"{0}: {1} - {2} - {3} - {4} - {5} - {6} - {7}\".format(i, trainS, testS, f1S, preS, recS, AUROC, model.best_params_))\n",
    "            print(delimsText)\n",
    "        if count >= max_count_RF:\n",
    "            break\n",
    "        res = pd.DataFrame(report, columns = columns)\n",
    "        mf.to_csv(res, \"./result/trained models/RF_models/RF_report.csv\")\n",
    "        mf.dump(model, \"./result/trained models/RF_models/\" + i + \"_RF.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Artificial Neural Network\n",
    "if use_ANN:\n",
    "    report = []\n",
    "    count = 0\n",
    "    for i in outcome_variables:\n",
    "        if count >= max_count_ANN:\n",
    "                print(\"Aborted by max_count equaled\", max_count_ANN)\n",
    "                break\n",
    "        count += 1\n",
    "        parameters = par_ANN\n",
    "        #model = GridSearchCV(RandomForestClassifier(), parameters, cv = 5)\n",
    "        model = GridSearchCV(MLPClassifier(), parameters, cv = 5)\n",
    "        #model = RandomForestClassifier(class_weight = \"balanced\")\n",
    "        model.fit(train[train_variables], train[i])\n",
    "        \n",
    "        y_score = model.predict_proba(test[train_variables])[:,1]\n",
    "        fpr, tpr, _ = roc_curve(np.array(test[i]), np.array(y_score), pos_label = 2)\n",
    "        AUROC = auc(fpr, tpr)\n",
    "        trainS = model.score(train[train_variables], train[i])\n",
    "        testS = model.score(test[train_variables], test[i])\n",
    "        f1S = metrics.f1_score(test[i], model.predict(test[train_variables]),average='micro')\n",
    "        preS = metrics.precision_score(test[i], model.predict(test[train_variables]),average='micro')\n",
    "        recS = metrics.recall_score(test[i], model.predict(test[train_variables]),average='micro')\n",
    "        AUROC = auc(fpr, tpr)\n",
    "        #feature_importance=pd.DataFrame({'column':train_variables, 'values':model.best_estimator_.coef_})\n",
    "        #feature_importance.sort_values(by='values', axis=0, ascending=False, inplace=True)\n",
    "        report.append([trainS, testS, f1S, preS, recS, AUROC, model.best_params_, i])\n",
    "        columns = [\"trainS\", \"testS\", \"f1S\", \"preS\", \"recS\", \"AUROC\", \"model.best_params_\", \"i\"]\n",
    "        #feature_importance.iloc[0].values,feature_importance.iloc[1].values,feature_importance.iloc[2].values,\n",
    "        #feature_importance.iloc[3].values,feature_importance.iloc[4].values])\n",
    "        print(\"{0}: {1} - {2} - {3} - {4} - {5} - {6} - {7}\".format(i, trainS, testS, f1S, preS, recS, AUROC, model.best_params_))\n",
    "        print(delimsText)\n",
    "        res = pd.DataFrame(report, columns = columns)\n",
    "        mf.to_csv(res, \"./result/trained models/ANN_models/report.csv\")\n",
    "        mf.dump(model, \"./result/trained models/ANN_models/\" + i + \"_ANN.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost\n",
    "if use_XGB:\n",
    "    for time, scale_pos_weight in enumerate(enumerate_lists):\n",
    "        for i in outcome_variables:\n",
    "            if count >= max_count_XGB:\n",
    "                print(\"Aborted by max_count equaled\", max_count_XGB)\n",
    "                break\n",
    "            count += 1\n",
    "            parameters = par_XGB[0]\n",
    "            parameters.update({'scale_pos_weight':[scale_pos_weight]})\n",
    "            parameters = [parameters]\n",
    "            model = GridSearchCV(XGBClassifier(n_jobs = -1), parameters, cv = 5)\n",
    "            model.fit(train[train_variables], train[i])\n",
    "            y_score = model.predict_proba(test[train_variables])[:,1]\n",
    "            fpr, tpr, _ = roc_curve(np.array(test[i]), np.array(y_score), pos_label = 2)\n",
    "            AUROC = auc(fpr, tpr)\n",
    "            trainS = model.score(train[train_variables], train[i])\n",
    "            testS = model.score(test[train_variables], test[i])\n",
    "            f1S = metrics.f1_score(test[i], model.predict(test[train_variables]), average = \"micro\")\n",
    "            preS = metrics.precision_score(test[i], model.predict(test[train_variables]), average = \"micro\")\n",
    "            recS = metrics.recall_score(test[i], model.predict(test[train_variables]), average = \"micro\")\n",
    "            AUROC = auc(fpr, tpr)\n",
    "            feature_importance = pd.DataFrame({\"column\":train_variables, \"values\":model.best_estimator_.feature_importances_})\n",
    "            feature_importance.sort_values(by = \"values\", axis = 0, ascending = False, inplace = True)\n",
    "            report.append([trainS, testS, f1S, preS, recS, AUROC, model.best_params_, i,\n",
    "                          feature_importance.iloc[0].values, feature_importance.iloc[1].values, feature_importance.iloc[2].values,\n",
    "                          feature_importance.iloc[3].values, feature_importance.iloc[4].values, scale_pos_weight])\n",
    "            columns = [\"trainS\", \"testS\", \"f1S\", \"preS\", \"recS\", \"AUROC\", \"model.best_params_\", \"i\",\n",
    "                        \"feature_importance.iloc[0].values\", \"feature_importance.iloc[1].values\", \"feature_importance.iloc[2].values\",\n",
    "                        \"feature_importance.iloc[3].values\", \"feature_importance.iloc[4].values\", \"scale_pos_weight\"]\n",
    "            print(\"{0}. scale: {1}\\n{2} - {3} - {4} - {5} - {6} - {7} - {8}\".format(i, scale_pos_weight, trainS, testS, f1S, preS, recS, AUROC, model.best_params_))\n",
    "            print(delimsText)\n",
    "        if count >= max_count_XGB:\n",
    "            break\n",
    "        res = pd.DataFrame(report, columns = columns)\n",
    "        mf.to_csv(res, \"./result/trained models/XGB_models/report.csv\")\n",
    "        mf.dump(model, \"./result/trained models/XGB_models/\" + i + \"_XGB.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC curve\n",
    "for i in range(len(name)):\n",
    "    plt.figure(dpi = 500)\n",
    "    modelXGB = joblib.load('./result/trained models/XGB_models/' + test_name[i] + '_XGB.model')\n",
    "    y_score = modelXGB.predict_proba(test[train_variables])[:, 1]\n",
    "    fpr, tpr, _ = roc_curve(test[test_name[i]], y_score)\n",
    "    AUROC = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, color = 'blue', label = 'XGB (AUROC = %0.2f)' % AUROC)\n",
    "    modelANN = joblib.load('./result/trained models/ANN_models/' + test_name[i] + '_ANN.model')\n",
    "    y_score = modelANN.predict_proba(test[train_variables])[:,1]\n",
    "    fpr, tpr, _ = roc_curve(test[test_name[i]], y_score)\n",
    "    AUROC = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, color = 'darkorange', label = 'ANN (AUROC = %0.2f)'%AUROC)\n",
    "    modelLR = joblib.load('./result/trained models/LR_models/' + test_name[i] + '_LR.model')\n",
    "    y_score = modelLR.predict_proba(test[train_variables])[:,1]\n",
    "    fpr, tpr, _ = roc_curve(test[test_name[i]], y_score)\n",
    "    AUROC=auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, color = \"red\", label = \"LR (AUROC = %0.2f)\" % AUROC)\n",
    "    modelRF = joblib.load(\"./result/trained models/RF_models/\" + test_name[i] + \"_RF.model\")\n",
    "    y_score = modelRF.predict_proba(test[train_variables])[:,1]\n",
    "    fpr, tpr, _ = roc_curve(test[test_name[i]], y_score)\n",
    "    AUROC = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, color = 'green', label = 'RF (AUROC = %0.2f)' % AUROC)\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.plot([0, 1], [0, 1], color = \"navy\", linestyle = \"--\")\n",
    "    plt.title(\"ROC curve of \" + name[i] + \" model\")\n",
    "    plt.legend(loc = \"lower right\")\n",
    "    plt.savefig(\"./result/ROC curve/\" + name[i] + \".jpg\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#score table\n",
    "model_name = [\"XGB\", \"ANN\", \"LR\", \"RF\"]\n",
    "res = []\n",
    "for i in range(len(test_name)):\n",
    "    modelXGB = joblib.load('result/trained models/XGB_models/' + test_name[i] + '_XGB.model')\n",
    "    modelANN = joblib.load('result/trained models/ANN_models/' + test_name[i] + '_ANN.model')\n",
    "    modelLR = joblib.load('result/trained models/LR_models/' + test_name[i] + '_LR.model')\n",
    "    modelRF = joblib.load('result/trained models/RF_models/' + test_name[i] + '_RF.model')\n",
    "    model_list = [modelXGB, modelANN, modelLR, modelRF]\n",
    "    for j in range(len(model_list)):\n",
    "        y_score = model_list[j].predict_proba(test[train_variables])[:,1]\n",
    "        fpr, tpr, _ = roc_curve(test[test_name[i]], y_score)\n",
    "        trainS = model_list[j].score(train[train_variables], train[test_name[i]])\n",
    "        testS = model_list[j].score(test[train_variables], test[test_name[i]])\n",
    "        f1S = metrics.f1_score(test[test_name[i]], model_list[j].predict(test[train_variables]))\n",
    "        preS = metrics.precision_score(test[test_name[i]], model_list[j].predict(test[train_variables]))\n",
    "        recS = metrics.recall_score(test[test_name[i]], model_list[j].predict(test[train_variables]))\n",
    "        AUROC = auc(fpr, tpr)\n",
    "        #print(trainS, '-', testS, '-', f1S, '-', preS, '-', recS, '-', AUROC)\n",
    "        #print(metrics.confusion_matrix(test[i], model.predict(test[train_variables])))\n",
    "        res.append([trainS, testS, f1S, preS, recS, AUROC, name[i], model_name[j]])  \n",
    "res = pd.DataFrame(res, columns = columns)\n",
    "mf.to_csv(res, \"result/ROC curve/score table.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost_FIN\n",
    "if use_FIN:\n",
    "    report = []\n",
    "    count = 0\n",
    "    \n",
    "    for time,scale_pos_weight in enumerate([30]):\n",
    "        for i in outcome_variables:\n",
    "            if count >= max_count_FIN:\n",
    "                print(\"Aborted by max_count equaled\", max_count)\n",
    "                break\n",
    "            count += 1\n",
    "            parameters = [par_FIN[0].update({'scale_pos_weight':[scale_pos_weight]})]\n",
    "            model = GridSearchCV(XGBClassifier(n_jobs = -1), parameters, cv = 5)\n",
    "            model.fit(train[train_variables], train[i])\n",
    "            y_score=model.predict_proba(test[train_variables])[:,1]\n",
    "            fpr, tpr, _ = roc_curve(test[i], y_score)\n",
    "            AUROC = auc(fpr, tpr)\n",
    "            trainS = model.score(train[train_variables], train[i])\n",
    "            testS = model.score(test[train_variables], test[i])\n",
    "            f1S = metrics.f1_score(test[i], model.predict(test[train_variables]))\n",
    "            preS = metrics.precision_score(test[i], model.predict(test[train_variables]))\n",
    "            recS = metrics.recall_score(test[i], model.predict(test[train_variables]))\n",
    "            AUROC = auc(fpr, tpr)\n",
    "            feature_importance = pd.DataFrame({\"column\":train_variables, \"values\":model.best_estimator_.feature_importances_})\n",
    "            feature_importance.sort_values(by = \"values\", axis = 0, ascending = False, inplace = True)\n",
    "            report.append([trainS, testS, f1S, preS,recS,AUROC,model.best_params_, i,\n",
    "                          feature_importance.iloc[0].values,feature_importance.iloc[1].values,feature_importance.iloc[2].values,\n",
    "                          feature_importance.iloc[3].values,feature_importance.iloc[4].values,scale_pos_weight])\n",
    "            columns = [\"trainS\", \"testS\", \"f1S\", \"preS\", \"recS\", \"AUROC\", \"model.best_params_\", \"i\",\n",
    "                        \"feature_importance.iloc[0].values\", \"feature_importance.iloc[1].values\", \"feature_importance.iloc[2].values\",\n",
    "                        \"feature_importance.iloc[3].values\", \"feature_importance.iloc[4].values\", \"scale_pos_weight\"]\n",
    "            print(\"{0}: \\nscale:{1}\".format(i, scale_pos_weight))\n",
    "            print(\" - \".join([trainS, testS, f1S, preS, recS, AUROC, model.best_params_]))\n",
    "            print(\"-\" * 30)\n",
    "        if count >= max_count_FIN:\n",
    "            break\n",
    "        res = pd.DataFrame(report, columns = columns)\n",
    "        mf.to_csv(res, \"./result/trained models/XGB_models/report_fin.csv\")\n",
    "        mf.dump(model, \"./result/trained models/XGB_models/\" + i + \"_XGB.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(y_name)):\n",
    "    model = joblib.load(file_name[i])\n",
    "    y_score = model.predict_proba(test[train_variables])[:,1]\n",
    "    fpr, tpr, _ = roc_curve(test[y_name[i]], y_score)\n",
    "    AUROC = auc(fpr, tpr)\n",
    "    trainS = model.score(train[train_variables], train[y_name[i]])\n",
    "    testS = model.score(test[train_variables], test[y_name[i]])\n",
    "    f1S = metrics.f1_score(test[y_name[i]], model.predict(test[train_variables]))\n",
    "    preS = metrics.precision_score(test[y_name[i]], model.predict(test[train_variables]))\n",
    "    recS = metrics.recall_score(test[y_name[i]], model.predict(test[train_variables]))\n",
    "    AUROC = auc(fpr, tpr)\n",
    "    print(\"{0}: {1} - {2} - {3} - {4} - {5} - {6} - {7}\\n\".format(y_name[i], trainS, testS, f1S, preS, recS, AUROC, model.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = joblib.load(\"./result/trained models/XGB_models/hospital_mortality_XGB.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.best_estimator_.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = pd.DataFrame({\"column\":train_variables, \"values\":model.best_estimator_.feature_importances_})\n",
    "feature_importance.sort_values(by = \"values\", axis = 0, ascending = False, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
